{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KefopSGHX1LG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdpJFZ0a8_yW",
        "outputId": "a912f63f-fe18-4ece-9f09-7b6a2ceef385"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "MAPA INICIAL:\n",
            "[  10.0] [‚Üë  0.0] [‚Üë  0.0] \n",
            "[‚Üë  0.0] [‚Üë  0.0] [ -10.0] \n",
            "[‚Üë  0.0] [‚Üë  0.0] [‚Üë  0.0] \n",
            "\n",
            "Ap√≥s 5 epis√≥dios:\n",
            "[  10.0] [‚Üë  0.0] [‚Üë  0.0] \n",
            "[‚Üë  6.8] [‚Üë  0.0] [ -10.0] \n",
            "[‚Üë  0.0] [‚Üë  0.0] [‚Üì  0.0] \n",
            "\n",
            "Ap√≥s 10 epis√≥dios:\n",
            "[  10.0] [‚Üë  0.0] [‚Üë  0.0] \n",
            "[‚Üë 11.2] [‚Üë  0.0] [ -10.0] \n",
            "[‚Üë  1.7] [‚Üë  0.0] [‚Üì  0.0] \n",
            "\n",
            "Ap√≥s 15 epis√≥dios:\n",
            "[  10.0] [‚Üê  6.8] [‚Üë  0.0] \n",
            "[‚Üë 14.0] [‚Üê  2.5] [ -10.0] \n",
            "[‚Üë  3.6] [‚Üë  0.1] [‚Üì  0.0] \n",
            "\n",
            "Ap√≥s 20 epis√≥dios:\n",
            "[  10.0] [‚Üê  9.3] [‚Üë  0.0] \n",
            "[‚Üë 16.5] [‚Üê  2.5] [ -10.0] \n",
            "[‚Üë  5.6] [‚Üë  0.1] [‚Üì  0.0] \n",
            "\n",
            "Ap√≥s 25 epis√≥dios:\n",
            "[  10.0] [‚Üê  9.3] [‚Üë  0.0] \n",
            "[‚Üë 17.7] [‚Üê  5.1] [ -10.0] \n",
            "[‚Üë  7.5] [‚Üê  1.0] [‚Üì  0.0] \n",
            "\n",
            "Ap√≥s 30 epis√≥dios:\n",
            "[  10.0] [‚Üê 11.2] [‚Üê  1.7] \n",
            "[‚Üë 18.3] [‚Üê  9.1] [ -10.0] \n",
            "[‚Üë  9.2] [‚Üë  2.4] [‚Üê  0.2] \n",
            "\n",
            "MAPA FINAL:\n",
            "[  10.0] [‚Üê 11.2] [‚Üê  1.7] \n",
            "[‚Üë 18.3] [‚Üê  9.1] [ -10.0] \n",
            "[‚Üë  9.2] [‚Üë  2.4] [‚Üê  0.2] \n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import time\n",
        "\n",
        "class Aprendizado_por_reforco:\n",
        "    posicao_rec_pun = []\n",
        "\n",
        "    def __init__(self, greedy: float, taxa_desc: float, alpha: float = 0.1):\n",
        "        \"\"\"\n",
        "        Inicializa o agente de Q-Learning\n",
        "        greedy: taxa de explora√ß√£o (Œµ) - probabilidade de explorar\n",
        "        taxa_desc: fator de desconto (Œ≥) - valor futuro\n",
        "        alpha: taxa de aprendizado - quanto atualizar a cada passo\n",
        "        \"\"\"\n",
        "        self.greedy = greedy\n",
        "        self.taxa_desc = taxa_desc\n",
        "        self.alpha = alpha\n",
        "        self.posicao_rec_pun = []\n",
        "        self.mapa = None\n",
        "        self.acoes = [0, 1, 2, 3]  # cima, baixo, esquerda, direita\n",
        "        self.nomes_acoes = {0: \"CIMA\", 1: \"BAIXO\", 2: \"ESQUERDA\", 3: \"DIREITA\"}\n",
        "        self.simbolos = {0: \"‚Üë\", 1: \"‚Üì\", 2: \"‚Üê\", 3: \"‚Üí\"}\n",
        "\n",
        "    def gerar_mapa(self,\n",
        "                   quant_linhas: int,\n",
        "                   quant_colunas: int,\n",
        "                   quant_negativos: int,\n",
        "                   quant_positivos: int,\n",
        "                   valor_negativo: float,\n",
        "                   valor_positivo: float):\n",
        "        \"\"\"\n",
        "        Gera o mapa com recompensas positivas e negativas\n",
        "        Mantendo a estrutura original: cada c√©lula tem [cima, baixo, esq, dir]\n",
        "        \"\"\"\n",
        "        mapa = []\n",
        "\n",
        "        # Inicializa mapa com [0,0,0,0] para cada c√©lula\n",
        "        for linha in range(quant_linhas):\n",
        "            nova_linha = []\n",
        "            for coluna in range(quant_colunas):\n",
        "                nova_linha.append([0, 0, 0, 0])  # [cima, baixo, esquerda, direita]\n",
        "            mapa.append(nova_linha)\n",
        "\n",
        "        total_celulas = quant_linhas * quant_colunas\n",
        "        posicoes_disponiveis = [(i, j) for i in range(quant_linhas)\n",
        "                                         for j in range(quant_colunas)]\n",
        "\n",
        "        random.shuffle(posicoes_disponiveis)\n",
        "\n",
        "        # Adiciona valores negativos (puni√ß√µes)\n",
        "        valor_negativo_original = valor_negativo\n",
        "        valores_aplicados_negativos = []\n",
        "\n",
        "        print(f\"\\nüìâ Distribuindo {quant_negativos} puni√ß√µes de valor total {valor_negativo}:\")\n",
        "        for i in range(quant_negativos):\n",
        "            if i == quant_negativos - 1:\n",
        "                valor_aplicado = valor_negativo_original - sum(valores_aplicados_negativos)\n",
        "            else:\n",
        "                valor_max = abs(valor_negativo_original) - abs(sum(valores_aplicados_negativos))\n",
        "                if valor_max > 0:\n",
        "                    valor_aplicado = -random.randint(1, valor_max)\n",
        "                else:\n",
        "                    valor_aplicado = valor_negativo_original - sum(valores_aplicados_negativos)\n",
        "\n",
        "            linha, coluna = posicoes_disponiveis.pop()\n",
        "            self.posicao_rec_pun.append((linha, coluna))\n",
        "            # Nas c√©lulas especiais, todos os 4 valores s√£o iguais (recompensa/puni√ß√£o)\n",
        "            for acao in range(4):\n",
        "                mapa[linha][coluna][acao] = valor_aplicado\n",
        "            valores_aplicados_negativos.append(valor_aplicado)\n",
        "            print(f\"  Puni√ß√£o {i+1}: {valor_aplicado} na posi√ß√£o ({linha}, {coluna})\")\n",
        "\n",
        "        # Adiciona valores positivos (recompensas)\n",
        "        valor_positivo_original = valor_positivo\n",
        "        valores_aplicados_positivos = []\n",
        "\n",
        "        print(f\"\\nüìà Distribuindo {quant_positivos} recompensas de valor total {valor_positivo}:\")\n",
        "        for i in range(quant_positivos):\n",
        "            if i == quant_positivos - 1:\n",
        "                valor_aplicado = valor_positivo_original - sum(valores_aplicados_positivos)\n",
        "            else:\n",
        "                valor_max = valor_positivo_original - sum(valores_aplicados_positivos)\n",
        "                if valor_max > 0:\n",
        "                    valor_aplicado = random.randint(1, valor_max)\n",
        "                else:\n",
        "                    valor_aplicado = valor_positivo_original - sum(valores_aplicados_positivos)\n",
        "\n",
        "            linha, coluna = posicoes_disponiveis.pop()\n",
        "            self.posicao_rec_pun.append((linha, coluna))\n",
        "            # Nas c√©lulas especiais, todos os 4 valores s√£o iguais (recompensa/puni√ß√£o)\n",
        "            for acao in range(4):\n",
        "                mapa[linha][coluna][acao] = valor_aplicado\n",
        "            valores_aplicados_positivos.append(valor_aplicado)\n",
        "            print(f\"  Recompensa {i+1}: +{valor_aplicado} na posi√ß√£o ({linha}, {coluna})\")\n",
        "\n",
        "        self.mapa = mapa\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"‚úÖ MAPA GERADO COM SUCESSO!\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        return mapa\n",
        "\n",
        "    def inicio_mapa(self, mapa):\n",
        "        \"\"\"Escolhe uma posi√ß√£o inicial aleat√≥ria que n√£o seja recompensa/puni√ß√£o\"\"\"\n",
        "        while True:\n",
        "            linha = random.randint(0, len(mapa) - 1)\n",
        "            coluna = random.randint(0, len(mapa[0]) - 1)\n",
        "\n",
        "            if (linha, coluna) not in self.posicao_rec_pun:\n",
        "                return (linha, coluna)\n",
        "\n",
        "    def mover(self, estado, acao):\n",
        "        \"\"\"\n",
        "        Executa uma a√ß√£o e retorna o novo estado\n",
        "        estado: tupla (linha, coluna)\n",
        "        acao: 0=cima, 1=baixo, 2=esquerda, 3=direita\n",
        "        \"\"\"\n",
        "        linha, coluna = estado\n",
        "\n",
        "        if acao == 0:  # cima\n",
        "            return (max(0, linha - 1), coluna)\n",
        "        elif acao == 1:  # baixo\n",
        "            return (min(len(self.mapa) - 1, linha + 1), coluna)\n",
        "        elif acao == 2:  # esquerda\n",
        "            return (linha, max(0, coluna - 1))\n",
        "        elif acao == 3:  # direita\n",
        "            return (linha, min(len(self.mapa[0]) - 1, coluna + 1))\n",
        "\n",
        "    def obter_recompensa(self, estado, acao=None):\n",
        "        \"\"\"\n",
        "        Retorna a recompensa do estado atual\n",
        "        Se a√ß√£o for fornecida, pega o valor espec√≠fico para aquela a√ß√£o\n",
        "        \"\"\"\n",
        "        linha, coluna = estado\n",
        "\n",
        "        if (linha, coluna) in self.posicao_rec_pun:\n",
        "            if acao is not None:\n",
        "                return self.mapa[linha][coluna][acao]\n",
        "            return self.mapa[linha][coluna][0]  # Todos s√£o iguais em terminais\n",
        "\n",
        "        # Para c√©lulas normais, retorna 0 (recompensa imediata)\n",
        "        return 0\n",
        "\n",
        "    def melhor_acao(self, estado):\n",
        "        \"\"\"Retorna a melhor a√ß√£o para um dado estado baseado no mapa\"\"\"\n",
        "        linha, coluna = estado\n",
        "        return max(range(4), key=lambda a: self.mapa[linha][coluna][a])\n",
        "\n",
        "    def escolher_acao(self, estado, episodio):\n",
        "        \"\"\"\n",
        "        Pol√≠tica Œµ-greedy: explora com probabilidade greedy,\n",
        "        explota com probabilidade 1-greedy\n",
        "        \"\"\"\n",
        "        if random.random() < self.greedy:\n",
        "            # Explora√ß√£o: a√ß√£o aleat√≥ria\n",
        "            return random.choice(self.acoes)\n",
        "        else:\n",
        "            # Explota√ß√£o: melhor a√ß√£o baseada nos valores atuais do mapa\n",
        "            return self.melhor_acao(estado)\n",
        "\n",
        "    def atualizar_valores(self, estado, acao, recompensa, novo_estado):\n",
        "        \"\"\"\n",
        "        Atualiza os valores do mapa usando a f√≥rmula do Q-Learning:\n",
        "        Q(s,a) = Q(s,a) + Œ± * [r + Œ≥ * max Q(s',a') - Q(s,a)]\n",
        "        \"\"\"\n",
        "        l, c = estado\n",
        "        nl, nc = novo_estado\n",
        "\n",
        "        valor_atual = self.mapa[l][c][acao]\n",
        "\n",
        "        # Pega o melhor valor do pr√≥ximo estado (max Q(s',a'))\n",
        "        valores_proximo = self.mapa[nl][nc]\n",
        "        max_futuro = max(valores_proximo)\n",
        "\n",
        "        # Calcula novo valor usando f√≥rmula do Q-Learning\n",
        "        novo_valor = valor_atual + self.alpha * (\n",
        "            recompensa + self.taxa_desc * max_futuro - valor_atual\n",
        "        )\n",
        "\n",
        "        # ATUALIZA O VALOR NO MAPA!\n",
        "        self.mapa[l][c][acao] = round(novo_valor, 2)\n",
        "\n",
        "        return novo_valor\n",
        "\n",
        "    def mostrar_mapa_compacto(self, titulo=\"\"):\n",
        "        \"\"\"Mostra o mapa de forma compacta para ver evolu√ß√£o\"\"\"\n",
        "        if self.mapa is None:\n",
        "            return\n",
        "\n",
        "        if titulo:\n",
        "            print(f\"\\n{titulo}\")\n",
        "\n",
        "        for i in range(len(self.mapa)):\n",
        "            linha_str = []\n",
        "            for j in range(len(self.mapa[0])):\n",
        "                valores = self.mapa[i][j]\n",
        "                if (i, j) in self.posicao_rec_pun:\n",
        "                    # C√©lula terminal\n",
        "                    if valores[0] > 0:\n",
        "                        linha_str.append(f\"[+{valores[0]:5.1f}]\")\n",
        "                    else:\n",
        "                        linha_str.append(f\"[{valores[0]:6.1f}]\")\n",
        "                else:\n",
        "                    # C√©lula normal mostra o maior valor (max Q)\n",
        "                    max_valor = max(valores)\n",
        "                    melhor_dir = self.simbolos[valores.index(max_valor)]\n",
        "                    linha_str.append(f\"[{melhor_dir}{max_valor:5.1f}]\")\n",
        "            print(\" \".join(linha_str))\n",
        "\n",
        "    def treinar_com_evolucao(self, episodios=50, max_passos=20, mostrar_a_cada=10):\n",
        "        \"\"\"\n",
        "        Treina e mostra a EVOLU√á√ÉO da matriz a cada N epis√≥dios\n",
        "        \"\"\"\n",
        "        if self.mapa is None:\n",
        "            raise ValueError(\"Gere o mapa primeiro usando gerar_mapa()!\")\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"üéØ INICIANDO TREINAMENTO COM VISUALIZA√á√ÉO DA EVOLU√á√ÉO\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        # Mostra estado inicial\n",
        "        self.mostrar_mapa_compacto(\"üìä MAPA INICIAL (antes do treinamento):\")\n",
        "\n",
        "        historico_recompensas = []\n",
        "\n",
        "        for ep in range(episodios):\n",
        "            estado = self.inicio_mapa(self.mapa)\n",
        "            recompensa_total = 0\n",
        "            passos = 0\n",
        "            episodio_concluido = False\n",
        "\n",
        "            while not episodio_concluido and passos < max_passos:\n",
        "                # Escolhe a√ß√£o\n",
        "                acao = self.escolher_acao(estado, ep)\n",
        "\n",
        "                # Executa a√ß√£o\n",
        "                novo_estado = self.mover(estado, acao)\n",
        "\n",
        "                # Observa recompensa\n",
        "                recompensa = self.obter_recompensa(novo_estado, acao)\n",
        "\n",
        "                # ATUALIZA OS VALORES NO MAPA!\n",
        "                valor_antigo = self.mapa[estado[0]][estado[1]][acao]\n",
        "                novo_valor = self.atualizar_valores(estado, acao, recompensa, novo_estado)\n",
        "\n",
        "                # Para depura√ß√£o, podemos ver atualiza√ß√µes individuais\n",
        "                if False:  # Mude para True se quiser ver cada atualiza√ß√£o\n",
        "                    print(f\"  Passo {passos}: Estado {estado}, A√ß√£o {self.nomes_acoes[acao]}, \"\n",
        "                          f\"Valor {valor_antigo:.2f} -> {novo_valor:.2f}, Recompensa {recompensa}\")\n",
        "\n",
        "                # Atualiza para pr√≥ximo passo\n",
        "                estado = novo_estado\n",
        "                recompensa_total += recompensa\n",
        "                passos += 1\n",
        "\n",
        "                if recompensa != 0:\n",
        "                    episodio_concluido = True\n",
        "\n",
        "            historico_recompensas.append(recompensa_total)\n",
        "\n",
        "            # Reduz explora√ß√£o gradualmente\n",
        "            if ep % 100 == 0 and ep > 0:\n",
        "                self.greedy = max(0.01, self.greedy * 0.95)\n",
        "\n",
        "            # MOSTRA A EVOLU√á√ÉO A CADA N EPIS√ìDIOS\n",
        "            if (ep + 1) % mostrar_a_cada == 0:\n",
        "                print(f\"\\nüìà Ap√≥s {ep+1} epis√≥dios (greedy={self.greedy:.2f}):\")\n",
        "                self.mostrar_mapa_compacto()\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"‚úÖ TREINAMENTO CONCLU√çDO!\")\n",
        "        print(f\"Recompensa m√©dia final: {sum(historico_recompensas[-10:])/10:.2f}\")\n",
        "\n",
        "        return historico_recompensas\n",
        "\n",
        "    def mostrar_mapa_detalhado(self, titulo=\"VALORES ATUAIS DO MAPA\"):\n",
        "        \"\"\"\n",
        "        Exibe os valores do mapa de forma detalhada (4 valores por c√©lula)\n",
        "        \"\"\"\n",
        "        if self.mapa is None:\n",
        "            print(\"Mapa n√£o gerado ainda!\")\n",
        "            return\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(f\"üìç {titulo}\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        # Cabe√ßalho\n",
        "        print(\"     \", end=\"\")\n",
        "        for j in range(len(self.mapa[0])):\n",
        "            print(f\"   Coluna {j}    \", end=\"\")\n",
        "        print()\n",
        "\n",
        "        # Linhas do mapa\n",
        "        for i in range(len(self.mapa)):\n",
        "            print(f\"L{i}   \", end=\"\")\n",
        "            for j in range(len(self.mapa[0])):\n",
        "                valores = self.mapa[i][j]\n",
        "                if (i, j) in self.posicao_rec_pun:\n",
        "                    # C√©lula terminal (recompensa/puni√ß√£o)\n",
        "                    print(f\"[{valores[0]:6.1f}] \", end=\"\")\n",
        "                else:\n",
        "                    # C√©lula normal mostra 4 valores\n",
        "                    print(f\"[‚Üë{valores[0]:4.1f} ‚Üì{valores[1]:4.1f} \", end=\"\")\n",
        "                    print(f\"‚Üê{valores[2]:4.1f} ‚Üí{valores[3]:4.1f}] \", end=\"\")\n",
        "            print()  # Nova linha\n",
        "\n",
        "        print(\"=\"*80)\n",
        "\n",
        "    def mostrar_politica(self):\n",
        "        \"\"\"Mostra a melhor a√ß√£o em cada estado baseado nos valores atuais\"\"\"\n",
        "        if self.mapa is None:\n",
        "            print(\"Mapa n√£o gerado ainda!\")\n",
        "            return\n",
        "\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"üéØ POL√çTICA ATUAL (melhor a√ß√£o por c√©lula):\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        for i in range(len(self.mapa)):\n",
        "            linha_politica = []\n",
        "            for j in range(len(self.mapa[0])):\n",
        "                if (i, j) in self.posicao_rec_pun:\n",
        "                    if self.obter_recompensa((i, j)) > 0:\n",
        "                        linha_politica.append(\"  ‚ö°  \")  # Recompensa\n",
        "                    else:\n",
        "                        linha_politica.append(\"  üíÄ  \")  # Puni√ß√£o\n",
        "                else:\n",
        "                    melhor_acao = self.melhor_acao((i, j))\n",
        "                    linha_politica.append(f\"  {self.simbolos[melhor_acao]}  \")\n",
        "            print(\"\".join(linha_politica))\n",
        "        print(\"=\"*50)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Configura√ß√£o do problema\n",
        "    print(\"ü§ñ Inicializando agente de Q-Learning...\")\n",
        "\n",
        "    # Cria agente com par√¢metros\n",
        "    rl = Aprendizado_por_reforco(\n",
        "        greedy=0.5,      # Taxa de explora√ß√£o alta no in√≠cio para ver mudan√ßas\n",
        "        taxa_desc=0.9,   # Fator de desconto\n",
        "        alpha=0.2        # Taxa de aprendizado mais alta para mudan√ßas vis√≠veis\n",
        "    )\n",
        "\n",
        "    # Gera um mapa PEQUENO para facilitar visualiza√ß√£o\n",
        "    print(\"üó∫Ô∏è Gerando mapa 3x3...\")\n",
        "    mapa = rl.gerar_mapa(\n",
        "        quant_linhas=6,\n",
        "        quant_colunas=6,\n",
        "        quant_negativos=2,   # 1 puni√ß√£o\n",
        "        quant_positivos=3,    # 1 recompensa\n",
        "        valor_negativo=-100,\n",
        "        valor_positivo=500\n",
        "    )\n",
        "\n",
        "    # Mostra o mapa INICIAL detalhado\n",
        "    rl.mostrar_mapa_detalhado(\"üìä MAPA INICIAL (valores detalhados)\")\n",
        "\n",
        "    # Mostra pol√≠tica inicial\n",
        "    rl.mostrar_politica()\n",
        "\n",
        "    # Treina MOSTRANDO A EVOLU√á√ÉO a cada 5 epis√≥dios\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"üîÑ INICIANDO TREINAMENTO COM EVOLU√á√ÉO VIS√çVEL\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    historico = rl.treinar_com_evolucao(\n",
        "        episodios=30,      # 30 epis√≥dios\n",
        "        max_passos=15,\n",
        "        mostrar_a_cada=5   # Mostra a cada 5 epis√≥dios\n",
        "    )\n",
        "\n",
        "    # Mostra o mapa FINAL detalhado\n",
        "    rl.mostrar_mapa_detalhado(\"üìä MAPA FINAL (ap√≥s treinamento)\")\n",
        "\n",
        "    # Mostra a pol√≠tica final\n",
        "    print(\"\\nüéØ Pol√≠tica final:\")\n",
        "    rl.mostrar_politica()\n",
        "\n",
        "    # Gr√°fico simples da evolu√ß√£o das recompensas\n",
        "    print(\"\\nüìà Evolu√ß√£o das recompensas por epis√≥dio:\")\n",
        "    print(\"-\" * 50)\n",
        "    for i, recomp in enumerate(historico):\n",
        "        if i % 5 == 0:  # Mostra a cada 5 epis√≥dios\n",
        "            bars = int((recomp + 10) * 2)  # Normaliza para visualiza√ß√£o\n",
        "            print(f\"Ep {i:2}: {recomp:5.1f} {'‚ñà' * max(0, bars)}\")"
      ]
    }
  ]
}